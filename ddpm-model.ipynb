{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 2.424365,
     "end_time": "2022-12-20T01:34:33.814564",
     "exception": false,
     "start_time": "2022-12-20T01:34:31.390199",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, random, torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.models.resnet import ResNet, BasicBlock\n",
    "from typing import Optional, Union, Iterable, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.023891,
     "end_time": "2022-12-20T01:34:33.870245",
     "exception": false,
     "start_time": "2022-12-20T01:34:33.846354",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Config Parameters\n",
    "data_dir = '/workstation/bhanu/npe'\n",
    "epochs = 200\n",
    "input_channels = 3\n",
    "first_fmap_channels = 64\n",
    "last_fmap_channels = 512\n",
    "output_channels = 3\n",
    "time_embedding = 256\n",
    "learning_rate = 1e-3\n",
    "min_lr = 1e-6\n",
    "weight_decay = 0.0\n",
    "n_timesteps = 1000\n",
    "beta_min = 1e-4\n",
    "beta_max = 2e-2\n",
    "beta_scheduler = 'linear'\n",
    "batch_size = 50\n",
    "image_size = (128, 128)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 10.658027,
     "end_time": "2022-12-20T01:34:44.560930",
     "exception": false,
     "start_time": "2022-12-20T01:34:33.902903",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#load samples\n",
    "img_pths = []\n",
    "for root, dirs, files in os.walk(data_dir):\n",
    "    for filename in files:\n",
    "        path = os.path.join(root, filename)\n",
    "        if path[-5:] == \".jpeg\":\n",
    "            img_pths.append(path)\n",
    "\n",
    "print(f'number of images: {len(img_pths)}')\n",
    "\n",
    "n_rows, n_cols = 2, 6\n",
    "fig, axs = plt.subplots(n_rows, n_cols, figsize=(17, 6))\n",
    "\n",
    "for i in range(n_rows):\n",
    "    for j in range(n_cols):\n",
    "        idx = random.randint(0, len(files))\n",
    "        img = Image.open(img_pths[idx])\n",
    "        axs[i, j].imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.035823,
     "end_time": "2022-12-20T01:34:44.649184",
     "exception": false,
     "start_time": "2022-12-20T01:34:44.613361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_pths, image_size):\n",
    "        self.img_pths = img_pths\n",
    "        self.image_size = image_size\n",
    "        \n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(self.image_size), \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_pths)\n",
    "    \n",
    "    def __getitem__(self, id):\n",
    "        if id >= len(self.img_pths):\n",
    "            raise StopIteration\n",
    "        try:\n",
    "            image = Image.open(self.img_pths[id])\n",
    "            image = self.transforms(image)\n",
    "            return image\n",
    "        except:\n",
    "            del self.img_pths[id]\n",
    "            return self.__getitem__(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.556948,
     "end_time": "2022-12-20T01:34:45.257001",
     "exception": false,
     "start_time": "2022-12-20T01:34:44.700053",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ImageDataset(img_pths, image_size)\n",
    "sample_img = dataset[0]\n",
    "\n",
    "print(f'shape: {sample_img.shape}')\n",
    "print(f'min pixel value: {sample_img.min()}')\n",
    "print(f'mean pixel value: {sample_img.mean()}')\n",
    "print(f'max pixel value: {sample_img.max()}')\n",
    "\n",
    "plt.imshow(sample_img.permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017187,
     "end_time": "2022-12-20T01:34:45.291881",
     "exception": false,
     "start_time": "2022-12-20T01:34:45.274694",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Diffusion Process\n",
    "\n",
    "The diffusion process consists of two parts, the forward process and the backward process, and both processes are on a single Markov chain.\n",
    "\n",
    "\n",
    "## The Forward Process\n",
    "In the **forward process**, noise is sampled from a normal gaussian distribution, this noise is then added to a sample image gradually in each timestep of the Markov chain. Mathematically, given an image $x_0$ and a noise $\\epsilon$, such that $\\epsilon$~$N(\\mu, I)$, given a Markov chain that spans $T$ timesteps, from $t=0$ to $t=T-1$, then the forward process $q$ can be defined as:\n",
    "$$q(x_{t+1}|x_{t}) = N(x_{t+1}; (\\sqrt{1-\\beta_{t+1}}) x_t, \\beta_{t+1}I)$$\n",
    "\n",
    "where:\n",
    "$\\beta$ is a sequence of scheduled values defined by two hyper-parameters, $\\beta_{min}$ and $\\beta_{max}$. In the original DDPM paper, $\\beta_{min}$ and $\\beta_{max}$ were defined to be 0.0004 and 0.02 respectively. The values in the $\\beta$ sequence correspond to each timestep in the markov chain. Assuming we have have a Markov chain with 3 timesteps:\n",
    "\n",
    "at $t=0$, $x = x_0$   (The original image)\n",
    "\n",
    "at $t=1$, $x = x_1 = q(x_1|x_0) = N(x_1; (\\sqrt{1-\\beta_{1}}) x_0, \\beta_1I)$\n",
    "\n",
    "at $t=2$, $x = x_2 = q(x_2|x_1) = N(x_2; (\\sqrt{1-\\beta_{2}}) x_1, \\beta_2I)$\n",
    "\n",
    "at $t=3$, $x = x_3 = q(x_3|x_2) = N(x_3; (\\sqrt{1-\\beta_{3}}) x_2, \\beta_3I)$\n",
    "\n",
    "Now, notice how the next timestep is only dependent on the immediate previous timestep, and nothing before, this is why the process chain is called a Markov chain.\n",
    "\n",
    "Now, we understand how the images are noised in each timestep of the forward process. However, during training, iteratively adding noise to the images in this manner is computationally expensive and hence, will take a very long time to run just a single training step. To avoid this computational cost, we introduce a term $\\alpha$, where $\\alpha = 1 - \\beta$. Now we can express the forward process interms of $\\alpha$ like so:\n",
    "\n",
    "$$q(x_{t+1}|x_t) = N(x_{t+1}; (\\sqrt{\\alpha_{t+1}}) x_t, (1-\\alpha_{t+1})I)$$\n",
    "\n",
    "Now, here comes the good part, assuming we have a sequence of $\\alpha$ values, $\\{\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, ... \\alpha_{T-1}\\}$ we can derive a term $\\hat{\\alpha}$, where $\\hat{\\alpha}$ is a sequence of the cummulative product of the terms in $\\{\\alpha_0, \\alpha_1, \\alpha_2, \\alpha_3, ... \\alpha_T\\}$, mathematically, $$\\hat{\\alpha} = \\prod_{t=0}^{T-1} \\alpha_t$$\n",
    "\n",
    "So, inotherwords, the first term in the $\\hat{\\alpha}$ sequence will be $\\hat{\\alpha}_0 = \\alpha_0$, the second term will be $\\hat{\\alpha}_1 = \\alpha_0 . \\alpha_1$, the T term will be $\\hat{\\alpha}_T = \\alpha_0 . \\alpha_1 . ... \\alpha_T$.\n",
    "\n",
    "The idea behind this is, if you want to apply noise to a given image up to a given timstep, you don't have to iteratively perform the computation, rather you can sample out the $\\hat{\\alpha}$ value for that given timestep $\\hat{\\alpha}_t$, and apply the noise with it to the image with the following formula:\n",
    "\n",
    "$$x_t := \\sqrt{\\hat{\\alpha}_t}.x_0 + \\sqrt{1-\\hat{\\alpha}_t}.\\epsilon$$\n",
    "\n",
    "And that is basically all their is to the forward process of the DDPM. *You can see all these play out in the `noiseImage` method of the `DiffusionUtils` class in the code cell below.*\n",
    "\n",
    "Now, pertaining how the $\\beta$ values are generated, these values can be generated by linear or cosine scheduling from $\\beta_{min}$ to $\\beta_{max}$. \n",
    "\n",
    "For linear scheduling, $\\beta_t$ is given as:\n",
    "\n",
    "$$\\beta_t = \\beta_{min} + t\\frac{\\beta_{max} - \\beta_{min}} {T}$$\n",
    "\n",
    "and the cosine scheduling will be given as:\n",
    "\n",
    "$$\\beta_t = \\beta_{min} + 0.5(\\beta_{max} - \\beta_{min}) (1 - \\cos{\\frac {t} {T}\\pi})$$\n",
    "\n",
    "Where $t$ ~ $Uniform(0, T-1)$\n",
    "\n",
    "The beta scheduling is done in the `betaSamples` method of the `DiffusionUtils` class. Depending on how you intialise the class, you can either schedule with 'linear' or 'cosine' technique, by specifying 'linear' or 'cosine' as the `scheduler` argument of the class `__init__` method. You can also pass the number of timesteps with the `n_timesteps` argument, as well as $beta_{min}$ and $beta_{max}$ with the `beta_min` and `beta_max` argument respectively.\n",
    "\n",
    "**Note:** In this explaination, all indexing started from 0 ($t=0$) and ends at $T-1$, unlike in the original paper where indexing beings at 1 ($t=1$) and ends with T. This is because we will be programming these mathematical functionalities, and in programming, computers start indexing a sequence at 0. So in the original paper, you might see the forward expression as: $q(x_t|x_{t-1}) = N(x_t; (\\sqrt{1-\\beta_t}) x_{t-1}, \\beta_tI)$, where  $t$ ~ $Uniform(1, T)$, which is equivalent to the expression in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017264,
     "end_time": "2022-12-20T01:34:45.326824",
     "exception": false,
     "start_time": "2022-12-20T01:34:45.309560",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# The Backward Process\n",
    "\n",
    "The backward process is the opposite of the forward process, in the sense that we attempt to reverse the noising of the forward process, inotherwords, we attempt to denoise, so as to get the inital image at the first timestep $t=0$, from the last timestep of the forward process $t=T-1$. One thing to note is that at the end of the forward process, it is assumed that $x_{T-1}$ is almost identical to pure noise sampled from a normal gaussian distribution, hence you could say; ($x_{T-1}$ ~ $N(\\mu, I)$). This is very intuitive, because by the time the parameterized model that is responsible for denoising in the backward process, learns to successfully convert a sample from $x_{T-1}$ to $x_0$, the model would more or less be able to generate real images from a data distribution, merely from pure random noise sampled from a normal gaussian distribution.\n",
    "\n",
    "In the backward process, we have a parameterized model (a neural network model), this model is responsible for predicting the entire noise added to the image. When the model predicts a noise, a portion of that predicted noise is removed form the image, depending the given timestep. The parameterized model takes in two inputs, the noised image and the corresponding timestep of the Markov chain, and predicts the noise added to the image. \n",
    "\n",
    "If we denote backward process as $p$, then $p$ can be mathematically expressed as:\n",
    "\n",
    "$$p(x_t | x_{t+1}) = N(x_t; \\mu(x_{t+1}, t+1), {\\sigma_0(x_{t+1}, t+1)}^2)$$\n",
    "\n",
    "Where, $\\mu(x_{t+1}, t+1)$ and ${\\sigma_0(x_{t+1}, t+1)}^2$ are the mean and variance of the predicted noise $\\epsilon(x_{t+1}, t+1))$.\n",
    "\n",
    "Now, let us express this in a more understandable way to get the intuition behind it. We've expressed the backward process mathematically, what exactly is the expression for $x_t$, given $x_{t+1}$? Well, the expression is simply the reverse of the expression of the forward process, except now, the noise being removed is the noise that the model predicted. The expression is as follows:\n",
    "\n",
    "$$x_t = \\frac 1 {\\sqrt{\\alpha_{t+1}}} (x_{t+1} - \\frac {1-\\alpha_{t+1}} {\\sqrt{1 - \\hat{\\alpha}_{t+1}}} \\epsilon(x_{t+1}, t+1)) + \\sigma_{t+1} z$$\n",
    "\n",
    "Where $\\sigma_t = \\sqrt{\\beta_t}$ : (standard deviation of noise) and $z$~$N(0, I)$.\n",
    "\n",
    "And that is pretty much it for the backward process.\n",
    "\n",
    "Now, one thing to note is that for $t=0$, $z$ is set to 0, this is because at $t=0$, $x_t = x_0$ which the original image without noise, hence their is no need adding the $z$ term to it.\n",
    "\n",
    "You can see all these play out in the `sample` method of the `DiffusionUtils` class, that takes in a batch of sampled noise and the trained model as arguments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.017324,
     "end_time": "2022-12-20T01:34:45.361730",
     "exception": false,
     "start_time": "2022-12-20T01:34:45.344406",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The `sampleTimestep` method randomly samples a batch of timsteps from 1 to $T$ (`n_timesteps`). This way, the sample image does not have to be completely noised all the way to the final timestep $T$ before the model can train, the model just needs to be able to perform denoising at any given timestep.\n",
    "\n",
    "In the later sections, we will discuss about how the model keeps track of timesteps, via the transformer sinusoidal embedding.\n",
    "\n",
    "The cell below is the full code of the diffusion processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.293768,
     "end_time": "2022-12-20T01:34:45.672961",
     "exception": false,
     "start_time": "2022-12-20T01:34:45.379193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionUtils:\n",
    "    def __init__(self, n_timesteps, beta_min, beta_max, device='cuda', scheduler='linear'):\n",
    "        assert scheduler in ['linear', 'cosine'], 'scheduler must be linear or cosine'\n",
    "\n",
    "        self.n_timesteps = n_timesteps\n",
    "        self.beta_min = beta_min\n",
    "        self.beta_max = beta_max\n",
    "        self.device = device\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "        self.betas = self.betaSamples()\n",
    "        self.alphas = 1 - self.betas\n",
    "        self.alpha_hat = torch.cumprod(self.alphas, dim=0)\n",
    "    \n",
    "    \n",
    "    def betaSamples(self):\n",
    "        if self.scheduler == 'linear':\n",
    "            return torch.linspace(start=self.beta_min, end=self.beta_max, steps=self.n_timesteps).to(self.device)\n",
    "\n",
    "        elif self.scheduler == 'cosine':\n",
    "            betas = []\n",
    "            for i in reversed(range(self.n_timesteps)):\n",
    "                T = self.n_timesteps - 1\n",
    "                beta = self.beta_min + 0.5*(self.beta_max - self.beta_min) * (1 + np.cos((i/T) * np.pi))\n",
    "                betas.append(beta)\n",
    "                \n",
    "            return torch.Tensor(betas).to(self.device)\n",
    "    \n",
    "    \n",
    "    def sampleTimestep(self, size):\n",
    "        return torch.randint(low=1, high=self.n_timesteps, size=(size, )).to(self.device)\n",
    "    \n",
    "    \n",
    "    def noiseImage(self, x, t):\n",
    "        assert len(x.shape) == 4, 'input must be 4 dimensions'\n",
    "        alpha_hat_sqrts = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        one_mins_alpha_hat_sqrt = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        noise = torch.randn_like(x).to(self.device)\n",
    "        return (alpha_hat_sqrts * x) + (one_mins_alpha_hat_sqrt * noise), noise\n",
    "    \n",
    "    def sample(self, x, model):\n",
    "        assert len(x.shape) == 4, 'input must be 4 dimensions'\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            iterations = range(1, self.n_timesteps)\n",
    "            for i in tqdm(reversed(iterations)):\n",
    "                #batch of timesteps t\n",
    "                t = (torch.ones(x.shape[0]) * i).long().to(self.device)\n",
    "                \n",
    "                #params\n",
    "                alpha = self.alphas[t][:, None, None, None]\n",
    "                beta = self.betas[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                one_minus_alpha = 1 - alpha\n",
    "                one_minus_alpha_hat = 1 - alpha_hat\n",
    "                \n",
    "                #predict noise pertaining for a given timestep\n",
    "                predicted_noise = model(x, t)\n",
    "                \n",
    "                if i > 1:noise = torch.randn_like(x).to(self.device)\n",
    "                else:noise = torch.zeros_like(x).to(self.device)\n",
    "                \n",
    "                x = 1/torch.sqrt(alpha) * (x - ((one_minus_alpha / torch.sqrt(one_minus_alpha_hat)) * predicted_noise))\n",
    "                x = x + (torch.sqrt(beta) * noise)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2.244464,
     "end_time": "2022-12-20T01:34:47.971722",
     "exception": false,
     "start_time": "2022-12-20T01:34:45.727258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "T = n_timesteps\n",
    "n_steps = 50\n",
    "alpha_values = {}\n",
    "\n",
    "for scheduler in ['linear', 'cosine']:\n",
    "    print(f'{scheduler} beta scheduling...')\n",
    "\n",
    "    diffusion = DiffusionUtils(T, beta_min, beta_max, device=DEVICE, scheduler=scheduler)\n",
    "    alpha_values[scheduler] = diffusion.alphas.cpu()\n",
    "\n",
    "    fig, axs = plt.subplots(1, (T//n_steps)+1, figsize=(25, 15))\n",
    "\n",
    "    axs[0].imshow(sample_img.permute(1, 2, 0))\n",
    "    axs[0].set_title('t = 0')\n",
    "\n",
    "    for idx, t in enumerate(range(n_steps-1, T, n_steps)):\n",
    "        t = torch.Tensor([t]).long()\n",
    "        x, _ = diffusion.noiseImage(sample_img.unsqueeze(0).to(DEVICE), t)\n",
    "        axs[idx+1].imshow(x.squeeze(0).permute(1, 2, 0).cpu())\n",
    "        axs[idx+1].set_title(f't = {t.item()}')\n",
    "    plt.show()\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.384202,
     "end_time": "2022-12-20T01:34:48.455608",
     "exception": false,
     "start_time": "2022-12-20T01:34:48.071406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(20, 4))\n",
    "\n",
    "axs[0].plot(alpha_values['linear'])\n",
    "axs[0].set_xlabel('timestep (t)')\n",
    "axs[0].set_ylabel('alpha (1-beta)')\n",
    "axs[0].set_title('alpha values of linear scheduling')\n",
    "\n",
    "axs[1].plot(alpha_values['cosine'])\n",
    "axs[1].set_xlabel('timestep (t)')\n",
    "axs[1].set_ylabel('alpha (1-beta)')\n",
    "axs[1].set_title('alpha values of cosine scheduling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.047536,
     "end_time": "2022-12-20T01:34:48.671665",
     "exception": false,
     "start_time": "2022-12-20T01:34:48.624129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SinusoidalEmbedding(nn.Module):\n",
    "    def __init__(self, dim_size, n:int=10000):\n",
    "        assert dim_size % 2 == 0, 'dim_size should be an even number'\n",
    "            \n",
    "        super(SinusoidalEmbedding, self).__init__()\n",
    "        \n",
    "        self.dim_size = dim_size\n",
    "        self.n = n\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        N = len(x)\n",
    "        output = torch.zeros(size=(N, self.dim_size)).to(x.device)\n",
    "        \n",
    "        for idx in range(0, N):\n",
    "            for i in range(0, self.dim_size//2):\n",
    "                emb = x[idx] / (self.n ** (2*i / self.dim_size))\n",
    "                output[idx, 2*i] = torch.sin(emb)\n",
    "                output[idx, (2*i) + 1] = torch.cos(emb)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.04655,
     "end_time": "2022-12-20T01:34:48.817523",
     "exception": false,
     "start_time": "2022-12-20T01:34:48.770973",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageSelfAttention(nn.Module):\n",
    "    def __init__(self, input_channels:int, n_heads:int):\n",
    "        super(ImageSelfAttention, self).__init__()\n",
    "        \n",
    "        self.input_channels = input_channels\n",
    "        self.n_heads = n_heads\n",
    "        self.layernorm = nn.LayerNorm(self.input_channels)\n",
    "        self.attention = nn.MultiheadAttention(self.input_channels, self.n_heads, batch_first=True)\n",
    "        \n",
    "    def forward(self, x:torch.Tensor):\n",
    "        # shape of x: (N, C, H, W)\n",
    "        _, C, H, W = x.shape\n",
    "        x = x.reshape(-1, C, H*W).permute(0, 2, 1)\n",
    "        normalised_x = self.layernorm(x)\n",
    "        attn_val, _ = self.attention(normalised_x, normalised_x, normalised_x)\n",
    "        attn_val = attn_val + x\n",
    "        attn_val = attn_val.permute(0, 2, 1).reshape(-1, C, H, W)\n",
    "        return attn_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.072485,
     "end_time": "2022-12-20T01:34:49.061720",
     "exception": false,
     "start_time": "2022-12-20T01:34:48.989235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoder(ResNet):\n",
    "    def __init__(\n",
    "        self, input_channels:int, time_embedding:int, \n",
    "        block=BasicBlock, block_layers:list=[2, 2, 2, 2], n_heads:int=4):\n",
    "      \n",
    "        self.block = block\n",
    "        self.block_layers = block_layers\n",
    "        self.time_embedding = time_embedding\n",
    "        self.input_channels = input_channels\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        super(Encoder, self).__init__(self.block, self.block_layers)\n",
    "        \n",
    "        #time embedding layer\n",
    "        self.sinusiodal_embedding = SinusoidalEmbedding(self.time_embedding)\n",
    "        \n",
    "        fmap_channels = [64, 64, 128, 256, 512]\n",
    "        #layers to project time embeddings unto feature maps\n",
    "        self.time_projection_layers = self.make_time_projections(fmap_channels)\n",
    "        #attention layers for each feature map\n",
    "        self.attention_layers = self.make_attention_layers(fmap_channels)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(\n",
    "            self.input_channels, 64, \n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3), \n",
    "            bias=False)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(\n",
    "            64, 64, \n",
    "            kernel_size=(7, 7), \n",
    "            stride=(2, 2), \n",
    "            padding=(3, 3),\n",
    "            bias=False)\n",
    "\n",
    "        #delete unwanted layers\n",
    "        del self.maxpool, self.fc, self.avgpool\n",
    "        \n",
    "        \n",
    "    def forward(self, x:torch.Tensor, t:torch.Tensor):\n",
    "        #embed time positions\n",
    "        t = self.sinusiodal_embedding(t)\n",
    "        \n",
    "        #prepare fmap2\n",
    "        fmap1 = self.conv1(x)\n",
    "        t_emb = self.time_projection_layers[0](t)\n",
    "        fmap1 = fmap1 + t_emb[:, :, None, None]\n",
    "        fmap1 = self.attention_layers[0](fmap1)\n",
    "        \n",
    "        x = self.conv2(fmap1)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        #prepare fmap2\n",
    "        fmap2 = self.layer1(x)\n",
    "        t_emb = self.time_projection_layers[1](t)\n",
    "        fmap2 = fmap2 + t_emb[:, :, None, None]\n",
    "        fmap2 = self.attention_layers[1](fmap2)\n",
    "        \n",
    "        #prepare fmap3\n",
    "        fmap3 = self.layer2(fmap2)\n",
    "        t_emb = self.time_projection_layers[2](t)\n",
    "        fmap3 = fmap3 + t_emb[:, :, None, None]\n",
    "        fmap3 = self.attention_layers[2](fmap3)\n",
    "        \n",
    "        #prepare fmap4\n",
    "        fmap4 = self.layer3(fmap3)\n",
    "        t_emb = self.time_projection_layers[3](t)\n",
    "        fmap4 = fmap4 + t_emb[:, :, None, None]\n",
    "        fmap4 = self.attention_layers[3](fmap4)\n",
    "        \n",
    "        #prepare fmap4\n",
    "        fmap5 = self.layer4(fmap4)\n",
    "        t_emb = self.time_projection_layers[4](t)\n",
    "        fmap5 = fmap5 + t_emb[:, :, None, None]\n",
    "        fmap5 = self.attention_layers[4](fmap5)\n",
    "        \n",
    "        return fmap1, fmap2, fmap3, fmap4, fmap5\n",
    "    \n",
    "    \n",
    "    def make_time_projections(self, fmap_channels:Iterable[int]):\n",
    "        layers = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(self.time_embedding, ch)\n",
    "            ) for ch in fmap_channels ])\n",
    "        \n",
    "        return layers\n",
    "    \n",
    "    def make_attention_layers(self, fmap_channels:Iterable[int]):\n",
    "        layers = nn.ModuleList([\n",
    "            ImageSelfAttention(ch, self.n_heads) for ch in fmap_channels\n",
    "        ])\n",
    "        \n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.069208,
     "end_time": "2022-12-20T01:34:49.245007",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.175799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self, input_channels:int, output_channels:int, \n",
    "        time_embedding:int, upsample_scale:int=2, activation:nn.Module=nn.ReLU,\n",
    "        compute_attn:bool=True, n_heads:int=4):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "\n",
    "        self.input_channels = input_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.upsample_scale = upsample_scale\n",
    "        self.time_embedding = time_embedding\n",
    "        self.compute_attn = compute_attn\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        #attention layer\n",
    "        if self.compute_attn:\n",
    "            self.attention = ImageSelfAttention(self.output_channels, self.n_heads)\n",
    "        else:self.attention = nn.Identity()\n",
    "        \n",
    "        #time embedding layer\n",
    "        self.sinusiodal_embedding = SinusoidalEmbedding(self.time_embedding)\n",
    "        \n",
    "        #time embedding projection layer\n",
    "        self.time_projection_layer = nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(self.time_embedding, self.output_channels)\n",
    "            )\n",
    "\n",
    "        self.transpose = nn.ConvTranspose2d(\n",
    "            self.input_channels, self.input_channels, \n",
    "            kernel_size=self.upsample_scale, stride=self.upsample_scale)\n",
    "        \n",
    "        self.instance_norm1 = nn.InstanceNorm2d(self.transpose.in_channels)\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            self.transpose.out_channels, self.output_channels, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        self.instance_norm2 = nn.InstanceNorm2d(self.conv.out_channels)\n",
    "        \n",
    "        self.activation = activation()\n",
    "\n",
    "    \n",
    "    def forward(self, fmap:torch.Tensor, prev_fmap:Optional[torch.Tensor]=None, t:Optional[torch.Tensor]=None):\n",
    "        output = self.transpose(fmap)\n",
    "        output = self.instance_norm1(output)\n",
    "        output = self.conv(output)\n",
    "        output = self.instance_norm2(output)\n",
    "        \n",
    "        #apply residual connection with previous feature map\n",
    "        if torch.is_tensor(prev_fmap):\n",
    "            assert (prev_fmap.shape == output.shape), 'feature maps must be of same shape'\n",
    "            output = output + prev_fmap\n",
    "            \n",
    "        #apply timestep embedding\n",
    "        if torch.is_tensor(t):\n",
    "            t = self.sinusiodal_embedding(t)\n",
    "            t_emb = self.time_projection_layer(t)\n",
    "            output = output + t_emb[:, :, None, None]\n",
    "            \n",
    "            output = self.attention(output)\n",
    "            \n",
    "        output = self.activation(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.062813,
     "end_time": "2022-12-20T01:34:49.420024",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.357211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(\n",
    "        self, last_fmap_channels:int, output_channels:int, \n",
    "        time_embedding:int, first_fmap_channels:int=64, n_heads:int=4):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.last_fmap_channels = last_fmap_channels\n",
    "        self.output_channels = output_channels\n",
    "        self.time_embedding = time_embedding\n",
    "        self.first_fmap_channels = first_fmap_channels\n",
    "        self.n_heads = n_heads\n",
    "\n",
    "        self.residual_layers = self.make_layers()\n",
    "\n",
    "        self.final_layer = DecoderBlock(\n",
    "            self.residual_layers[-1].input_channels, self.output_channels,\n",
    "            time_embedding=self.time_embedding, activation=nn.Identity, \n",
    "            compute_attn=False, n_heads=self.n_heads)\n",
    "\n",
    "        #set final layer second instance norm to identity\n",
    "        self.final_layer.instance_norm2 = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, *fmaps, t:Optional[torch.Tensor]=None):\n",
    "        #fmaps(reversed): fmap5, fmap4, fmap3, fmap2, fmap1\n",
    "        fmaps = [fmap for fmap in reversed(fmaps)]\n",
    "        ouptut = None\n",
    "        for idx, m in enumerate(self.residual_layers):\n",
    "            if idx == 0:\n",
    "                output = m(fmaps[idx], fmaps[idx+1], t)\n",
    "                continue\n",
    "            output = m(output, fmaps[idx+1], t)\n",
    "        \n",
    "        # no previous fmap is passed to the final decoder block\n",
    "        # and no attention is computed\n",
    "        output = self.final_layer(output)\n",
    "        return output\n",
    "\n",
    "      \n",
    "    def make_layers(self, n:int=4):\n",
    "        layers = []\n",
    "        for i in range(n):\n",
    "            if i == 0: in_ch = self.last_fmap_channels\n",
    "            else: in_ch = layers[i-1].output_channels\n",
    "\n",
    "            out_ch = in_ch // 2 if i != (n-1) else self.first_fmap_channels\n",
    "            layer = DecoderBlock(\n",
    "                in_ch, out_ch, \n",
    "                time_embedding=self.time_embedding,\n",
    "                compute_attn=True, n_heads=self.n_heads)\n",
    "            \n",
    "            layers.append(layer)\n",
    "\n",
    "        layers = nn.ModuleList(layers)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.04485,
     "end_time": "2022-12-20T01:34:49.563519",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.518669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DiffusionNet(nn.Module):\n",
    "    def __init__(self, encoder:Encoder, decoder:Decoder):\n",
    "        \n",
    "        super(DiffusionNet, self).__init__()\n",
    "        \n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "    \n",
    "    def forward(self, x:torch.Tensor, t:torch.Tensor):\n",
    "        enc_fmaps = self.encoder(x, t=t)\n",
    "        segmentation_mask = self.decoder(*enc_fmaps, t=t)\n",
    "        return segmentation_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035205,
     "end_time": "2022-12-20T01:34:49.631917",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.596712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Training Pipeline\n",
    "\n",
    "The training pipeline class is fairly straight forward, it takes in the model, loss function, optimizer and diffusion utils objects as input alongside other arugments like device and weight initialisation...\n",
    "\n",
    "The network weights are initialised with the `xavier_init_weight` or the `custom_weight_initializer`, when `weight_init` is True. The `save_model` method as the name implies is responsible for saving the model and optimization weights. The `train` method is where the model trains, it takes the dataloader as argument and loads the data in batches for training. In the training method, we sample a batch of timesteps $t$ with the `sampleTimestep` method of the `diffusion_utils` opbject, then for each of those timesteps, we compute the a batch of noised images with the `noiseImage` method, this method also returns the batch of noise alongside the noised images. The noised images are sent into the model alongside their corresponding timesteps, the output of the model is then compared to the noise added to the image via mean square error metric. Finally, the model gradients are computed by partially differentiating the MSE loss expression with respect to the model weights and then the model weights are computed, and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.052402,
     "end_time": "2022-12-20T01:34:49.717413",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.665011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, model, lossfunc, optimizer,scheduler, diffusion_utils, \n",
    "                 device='cuda', pretrained = False, weight_init=True):\n",
    "        \n",
    "        self.device = device\n",
    "        self.model = model.to(self.device)\n",
    "        self.lossfunc = lossfunc\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.weight_init = weight_init\n",
    "        self.diffusion_utils = diffusion_utils\n",
    "        self.best_loss = None\n",
    "        if self.weight_init:\n",
    "            self.model.apply(self.xavier_init_weights)\n",
    "        if pretrained:\n",
    "            self.load()\n",
    "\n",
    "    def xavier_init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n",
    "            nn.init.xavier_uniform_(m.weight)\n",
    "            if torch.is_tensor(m.bias):\n",
    "                m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def save(self,loss):\n",
    "        state = {\n",
    "            \"state_dict\": self.model.state_dict(),\n",
    "            \"optimizer\": self.optimizer.state_dict(),\n",
    "        }\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = loss\n",
    "            torch.save(state, \"/workstation/bhanu/neuralPhotoEditing/DDPM/ddpm_model_128_1000.pth\")\n",
    "            print(\"=>Checkpoint Saved\")\n",
    "        elif loss <= self.best_loss:\n",
    "            self.best_loss = loss\n",
    "            torch.save(state, \"/workstation/bhanu/neuralPhotoEditing/DDPM/ddpm_model_128_1000.pth\")\n",
    "            print(\"=>Checkpoint Saved\")\n",
    "            \n",
    "    def load(self):\n",
    "        print(\"=> Loading checkpoint\")\n",
    "        checkpoint = torch.load(\"/workstation/bhanu/neuralPhotoEditing/DDPM/models/ddpm_model_128_500.pth\")\n",
    "        self.model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        \n",
    "\n",
    "    def train(self, dataloader, epoch, verbose=False):\n",
    "        self.model.train()\n",
    "        loss = 0\n",
    "        for idx, images in enumerate(tqdm(dataloader)):\n",
    "            self.model.zero_grad()\n",
    "            images = images.to(self.device)\n",
    "            t = self.diffusion_utils.sampleTimestep(size=images.shape[0])\n",
    "            x_t, noise = self.diffusion_utils.noiseImage(images, t)\n",
    "            pred_noise = self.model(x_t, t)\n",
    "            batch_loss = self.lossfunc(pred_noise, noise)\n",
    "            batch_loss.backward()\n",
    "            self.optimizer.step()\n",
    "            loss += batch_loss.item()\n",
    "            \n",
    "        loss = loss / (idx + 1)\n",
    "        self.save(loss)\n",
    "        self.scheduler.step()\n",
    "        if verbose:\n",
    "            print(f'Epoch[{epoch}]: Training Loss: {loss}')\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def generate(self, epoch=1, n=4):\n",
    "        x = torch.randn(n, 3, *image_size).to(self.device)\n",
    "        generated_images = self.diffusion_utils.sample(x, self.model)\n",
    "        generated_images = generated_images.cpu()\n",
    "        generated_images = (generated_images.clamp(-1, 1) + 1) / 2\n",
    "\n",
    "        fig, axs = plt.subplots(1, n, figsize=(25, 15))\n",
    "        \n",
    "        for i in range(n):\n",
    "            img = generated_images[i].permute(1, 2, 0).numpy() * 255\n",
    "            img = img.astype(np.uint8)\n",
    "            axs[i].imshow(img)\n",
    "            axs[i].set_xticks([])\n",
    "            axs[i].set_yticks([])\n",
    "        plt.savefig(\"generation_{}.png\".format(epoch))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.298804,
     "end_time": "2022-12-20T01:34:50.179568",
     "exception": false,
     "start_time": "2022-12-20T01:34:49.880764",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#train dataset and dataloader\n",
    "train_dataset = ImageDataset(img_pths, image_size)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 3.848114,
     "end_time": "2022-12-20T01:34:54.297918",
     "exception": false,
     "start_time": "2022-12-20T01:34:50.449804",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#encoder, decoder model initialisation\n",
    "encoder = Encoder(input_channels, time_embedding, block_layers=[2, 2, 2, 2])\n",
    "decoder = Decoder(last_fmap_channels, output_channels, time_embedding, first_fmap_channels)\n",
    "model = DiffusionNet(encoder, decoder)\n",
    "\n",
    "#diffusion utilities class initialisaion\n",
    "diffusion_utils = DiffusionUtils(n_timesteps, beta_min, beta_max, device=DEVICE, scheduler=beta_scheduler)\n",
    "\n",
    "#loss function, optimizer and pipeline initialisation\n",
    "lossfunc = nn.MSELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "    optimizer, T_max=epochs, eta_min=min_lr, verbose=True\n",
    ")\n",
    "trainer = Trainer(model, lossfunc, optimizer,scheduler, diffusion_utils, device=DEVICE, pretrained = True, weight_init=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 11870.368611,
     "end_time": "2022-12-20T04:52:45.493410",
     "exception": false,
     "start_time": "2022-12-20T01:34:55.124799",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "for epoch in range(epochs):\n",
    "    trainer.generate(epoch)\n",
    "    train_loss = trainer.train(train_dataloader, epoch, verbose=True)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f'\\ntraining loss: {train_loss}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.generate(1,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
